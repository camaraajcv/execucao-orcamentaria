import streamlit as st
import requests
import zipfile
import io
import pandas as pd
from datetime import date

# ==========================
# CONFIG
# ==========================
st.set_page_config(page_title="Or√ßamento/Despesa 2026 ‚Äî UO 52111 e 52911", layout="wide")

FONTE_URL = "https://portaldatransparencia.gov.br/download-de-dados/orcamento-despesa/2026"
UOS_ALVO = {"52111", "52911"}  # manter como string pra bater com qualquer formata√ß√£o

# Coluna chave (como voc√™ descreveu)
COL_UO = "C√≥digo Unidade Or√ßament√°ria"

# ==========================
# FUN√á√ïES
# ==========================
@st.cache_data(show_spinner=False, ttl=60 * 60 * 24)  # cache 24h
def baixar_zip(url: str) -> bytes:
    """
    Baixa o ZIP do Portal (ou o arquivo que o link devolver).
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (StreamlitCloud)",
        "Accept": "*/*",
        "Referer": "https://portaldatransparencia.gov.br/",
    }
    r = requests.get(url, headers=headers, timeout=180)
    r.raise_for_status()
    return r.content

def achar_primeiro_csv_no_zip(zip_bytes: bytes) -> str:
    """
    Retorna o nome do primeiro arquivo .csv dentro do zip.
    """
    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:
        nomes = z.namelist()
        csvs = [n for n in nomes if n.lower().endswith(".csv")]
        if not csvs:
            raise RuntimeError(f"N√£o encontrei nenhum CSV dentro do ZIP. Arquivos: {nomes[:20]}")
        return csvs[0]

def ler_csv_filtrado_do_zip(zip_bytes: bytes, member_csv: str, uos_alvo: set[str], chunksize: int = 200_000) -> pd.DataFrame:
    """
    L√™ o CSV dentro do ZIP em chunks e filtra pelas UOs alvo.
    """
    # Tentativas de encoding e separador comuns em dados do governo
    encodings = ["utf-8-sig", "latin-1"]
    seps = [";", ","]

    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:
        with z.open(member_csv) as f:
            raw = f.read()  # l√™ pra mem√≥ria (se o CSV for gigante e estourar, eu te passo vers√£o streaming)
            bio = io.BytesIO(raw)

    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                # Reinicia o buffer a cada tentativa
                bio.seek(0)

                # Leitura em chunks para filtrar sem carregar tudo
                it = pd.read_csv(
                    bio,
                    sep=sep,
                    encoding=enc,
                    dtype=str,
                    chunksize=chunksize,
                    low_memory=False
                )

                partes = []
                for chunk in it:
                    if COL_UO not in chunk.columns:
                        raise RuntimeError(
                            f"Coluna '{COL_UO}' n√£o encontrada. Colunas dispon√≠veis: {list(chunk.columns)[:40]}"
                        )
                    # Normaliza UO como string sem espa√ßos
                    uo = chunk[COL_UO].astype(str).str.strip()
                    partes.append(chunk[uo.isin(uos_alvo)])

                df = pd.concat(partes, ignore_index=True) if partes else pd.DataFrame()
                return df

            except Exception as e:
                last_err = e

    raise RuntimeError(f"Falha ao ler o CSV. √öltimo erro: {last_err}")

def to_excel_bytes(df: pd.DataFrame) -> bytes:
    out = io.BytesIO()
    with pd.ExcelWriter(out, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="UO_52111_52911")
    return out.getvalue()


# ==========================
# UI
# ==========================
st.title("üì• Or√ßamento/Despesa 2026 ‚Äî filtro por Unidade Or√ßament√°ria")
st.write(f"Filtro aplicado: **{', '.join(sorted(UOS_ALVO))}**")

with st.sidebar:
    st.header("Par√¢metros")
    chunksize = st.selectbox("Tamanho do chunk (performance)", [50_000, 100_000, 200_000, 400_000], index=2)
    carregar = st.button("‚¨áÔ∏è Baixar ZIP e carregar dados", use_container_width=True)

    st.divider()
    st.caption("Fonte:")
    st.write(FONTE_URL)

if not carregar:
    st.info("Clique em **Baixar ZIP e carregar dados**.")
    st.stop()

with st.spinner("Baixando ZIP‚Ä¶"):
    zip_bytes = baixar_zip(FONTE_URL)

with st.spinner("Localizando CSV no ZIP‚Ä¶"):
    csv_name = achar_primeiro_csv_no_zip(zip_bytes)

st.success(f"CSV encontrado no ZIP: **{csv_name}**")

with st.spinner("Lendo CSV e filtrando por Unidade Or√ßament√°ria (em chunks)‚Ä¶"):
    df = ler_csv_filtrado_do_zip(zip_bytes, csv_name, UOS_ALVO, chunksize=int(chunksize))

if df.empty:
    st.warning("Nenhum registro encontrado para as Unidades Or√ßament√°rias informadas.")
else:
    st.success(f"Registros ap√≥s filtro: **{len(df):,}**".replace(",", "."))

# ==========================
# EXIBI√á√ÉO + DOWNLOADS
# ==========================
st.subheader("üìä Dados filtrados")
st.dataframe(df, use_container_width=True)

st.subheader("‚¨áÔ∏è Exportar")
st.download_button(
    "Baixar CSV filtrado",
    data=df.to_csv(index=False).encode("utf-8"),
    file_name="orcamento_despesa_2026_uo_52111_52911.csv",
    mime="text/csv",
)
st.download_button(
    "Baixar Excel filtrado",
    data=to_excel_bytes(df),
    file_name="orcamento_despesa_2026_uo_52111_52911.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)

# ==========================
# DICION√ÅRIO DE DADOS (resumo)
# ==========================
with st.expander("üìò Dicion√°rio de dados (resumo)"):
    st.markdown(
        """
**Colunas principais (conforme informado):**
- Exerc√≠cio
- C√≥digo/Nome √ìrg√£o Superior e Subordinado
- **C√≥digo/Nome Unidade Or√ßament√°ria**
- C√≥digo/Nome Fun√ß√£o e Subfun√ß√£o
- C√≥digo/Nome Programa Or√ßament√°rio
- C√≥digo/Nome A√ß√£o
- Categoria Econ√¥mica
- Grupo de Despesa (GND)
- Elemento de Despesa
- Or√ßamento Inicial (R$)
- Or√ßamento Atualizado (R$)
- Or√ßamento Empenhado (R$)
- Or√ßamento Realizado (R$)
- % Realizado do or√ßamento (Realizado/Atualizado * 100)
        """
    )

# ==========================
# RODAP√â (FONTE)
# ==========================
st.markdown("---")
st.caption(f"Fonte dos dados: {FONTE_URL}")
